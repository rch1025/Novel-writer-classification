{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LGB-tfidf .ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joonyoung-Song/DACON-NLP_competition/blob/main/LGB_tfidf_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fGTQqubQYf2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import re\n",
        "import os\n",
        "import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "import lightgbm as lgbm\n",
        "import xgboost as xgb\n",
        "\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn import ensemble, metrics, model_selection, naive_bayes\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "from keras.initializers import Constant\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem import PorterStemmer\n",
        "import string"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fExgsc2pytBn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p6MRqPpy5ct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b77b57-0088-4826-a822-07a559a2cee4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjk301MFzBbR"
      },
      "source": [
        "data_dir = Path('/gdrive/My Drive/dacon_nlp_competition/data')\n",
        "feature_dir = Path('/gdrive/My Drive/dacon_nlp_competition/build/feature')\n",
        "val_dir = Path('/gdrive/My Drive/dacon_nlp_competition/build/val')\n",
        "tst_dir = Path('/gdrive/My Drive/dacon_nlp_competition/build/tst')\n",
        "sub_dir = Path('/gdrive/My Drive/dacon_nlp_competition/build/sub')\n",
        "\n",
        "trn_file = data_dir / 'train.csv'\n",
        "tst_file = data_dir / 'test_x.csv'\n",
        "sample_file = data_dir / 'sample_submission.csv'\n",
        "\n",
        "target_col = 'author'\n",
        "n_fold = 5\n",
        "n_class = 5\n",
        "seed = 42"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMcsFNEozB8U"
      },
      "source": [
        "train = pd.read_csv(trn_file, encoding = 'utf-8')\n",
        "test = pd.read_csv(tst_file, encoding = 'utf-8')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RURrZsZB-PHr",
        "outputId": "077eea29-42f1-45a2-bd97-4952c8fa47b1"
      },
      "source": [
        "# 불용어 불러오기\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english')) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG_CZ9G3-PFq"
      },
      "source": [
        "## Number of words in the text ##\n",
        "train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "test[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "## Number of unique words in the text ##\n",
        "train[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
        "test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
        "\n",
        "## Number of characters in the text ##\n",
        "train[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\n",
        "test[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n",
        "\n",
        "## Number of stopwords in the text ##\n",
        "train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
        "test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
        "\n",
        "## Number of punctuations in the text ##\n",
        "train[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "test[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "\n",
        "## Number of title case words in the text ##\n",
        "train[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "\n",
        "## Number of title case words in the text ##\n",
        "train[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
        "test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
        "\n",
        "## Average length of the words in the text ##\n",
        "train[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "test[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFdbpcRP-O5W"
      },
      "source": [
        "## Prepare the data for modeling ###\n",
        "train_y = train['author']\n",
        "train_id = train['index'].values\n",
        "test_id = test['index'].values\n",
        "\n",
        "### recompute the trauncated variables again ###\n",
        "train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "test[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "train[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "test[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "\n",
        "cols_to_drop = ['index', 'text']\n",
        "train_X = train.drop(cols_to_drop + ['author'], axis=1)\n",
        "test_X = test.drop(cols_to_drop, axis=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmpdM0C3-O2O"
      },
      "source": [
        "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val = 42, child=1, colsample=0.3):\n",
        "    param = {}\n",
        "    param['objective'] = 'multi:softprob'\n",
        "    param['eta'] = 0.1\n",
        "    param['max_depth'] = 10\n",
        "    param['silent'] = 1\n",
        "    param['booster'] = 'dart'\n",
        "    param['num_class'] = 5\n",
        "    param['eval_metric'] = \"mlogloss\"\n",
        "    param['min_child_weight'] = child\n",
        "    param['subsample'] = 0.8\n",
        "    param['colsample_bytree'] = colsample\n",
        "    param['seed'] = seed_val\n",
        "    param['tree_method'] = 'gpu_hist'\n",
        "    num_rounds = 20000\n",
        "\n",
        "    plst = list(param.items())\n",
        "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
        "\n",
        "    if test_y is not None:\n",
        "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
        "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
        "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval= 100)\n",
        "    else:\n",
        "        xgtest = xgb.DMatrix(test_X)\n",
        "        model = xgb.train(plst, xgtrain, num_rounds)\n",
        "\n",
        "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
        "    if test_X2 is not None:\n",
        "        xgtest2 = xgb.DMatrix(test_X2)\n",
        "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
        "    return pred_test_y, pred_test_y2, model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BFNy3nvK4Gv"
      },
      "source": [
        "def runLGBM(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val = 42, child=1):\n",
        "    param = {}\n",
        "    param['objective'] = 'multiclass'\n",
        "    param['boosting_type'] = 'gbdt'\n",
        "    param['subsample_freq'] = 5\n",
        "    param['max_depth'] = 30\n",
        "    param['num_leaves'] = 100\n",
        "    param['num_class'] = 5\n",
        "    param['colsample_bytree']=0.7\n",
        "    param['subsample'] = 0.8\n",
        "    param['min_data_in_leaf'] = 64\n",
        "    param['metric'] = 'multi_logloss'\n",
        "    param['subsample_for_bin'] = 23000\n",
        "    param['min_child_weight'] = child\n",
        "    param['learning_rate'] = 0.01\n",
        "    param['seed'] = seed_val\n",
        "    n_estimators = 20000\n",
        "\n",
        "    # plst = param.items()\n",
        "    lgbmtrain = lgbm.Dataset(train_X, label=train_y)\n",
        "\n",
        "    if test_y is not None:\n",
        "        lgbmtest = lgbm.Dataset(test_X, label=test_y)\n",
        "        # watchlist = [ (lgbmtrain,'train'), (lgbmtest, 'test') ]\n",
        "        model = lgbm.train(param, lgbmtrain, n_estimators,valid_sets= [lgbmtrain,lgbmtest], early_stopping_rounds=50, verbose_eval= 100)\n",
        "    else:\n",
        "        lgbmtest = lgbm.Dataset(test_X)\n",
        "        model = lgbm.train(plst, lgbmtrain, num_rounds)\n",
        "\n",
        "    pred_test_y = model.predict(test_X)\n",
        "    if test_X2 is not None:\n",
        "        pred_test_y2 = model.predict(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv0VAAaK-OoE"
      },
      "source": [
        "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
        "    model = naive_bayes.MultinomialNB()\n",
        "    model.fit(train_X, train_y)\n",
        "    pred_test_y = model.predict_proba(test_X)\n",
        "    pred_test_y2 = model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LDB2rSQ-OrX"
      },
      "source": [
        "### Fit transform the tfidf vectorizer ###\n",
        "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
        "full_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75KixRk9D5RV"
      },
      "source": [
        "n_comp = 20\n",
        "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
        "svd_obj.fit(train_tfidf)\n",
        "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
        "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
        "    \n",
        "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
        "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
        "train_df = pd.concat([train, train_svd], axis=1)\n",
        "test_df = pd.concat([test, test_svd], axis=1)\n",
        "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl4imJ-ID5PI"
      },
      "source": [
        "### Fit transform the count vectorizer ###\n",
        "tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
        "tfidf_vec.fit(train_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acdwasGnFh9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45d208e-454c-4947-9f16-56d93e74642d"
      },
      "source": [
        "### Fit transform the tfidf vectorizer ###\n",
        "tfidf_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
        "tfidf_vec.fit(train_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 5])\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state = 42)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "# add the predictions as new features #\n",
        "train_df[\"nb_cvec_char_eap\"] = pred_train[:,0]\n",
        "train_df[\"nb_cvec_char_hpl\"] = pred_train[:,1]\n",
        "train_df[\"nb_cvec_char_mws\"] = pred_train[:,2]\n",
        "test_df[\"nb_cvec_char_eap\"] = pred_full_test[:,0]\n",
        "test_df[\"nb_cvec_char_hpl\"] = pred_full_test[:,1]\n",
        "test_df[\"nb_cvec_char_mws\"] = pred_full_test[:,2]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean cv score :  5.340966334159593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBqCyAIYD5IJ",
        "outputId": "16d2fe69-873f-4bc8-8d81-184aadb8b8f0"
      },
      "source": [
        "### Fit transform the tfidf vectorizer ###\n",
        "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\n",
        "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 5])\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state= 42)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "# add the predictions as new features #\n",
        "train_df[\"nb_tfidf_char_eap\"] = pred_train[:,0]\n",
        "train_df[\"nb_tfidf_char_hpl\"] = pred_train[:,1]\n",
        "train_df[\"nb_tfidf_char_mws\"] = pred_train[:,2]\n",
        "test_df[\"nb_tfidf_char_eap\"] = pred_full_test[:,0]\n",
        "test_df[\"nb_tfidf_char_hpl\"] = pred_full_test[:,1]\n",
        "test_df[\"nb_tfidf_char_mws\"] = pred_full_test[:,2]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean cv score :  1.6966907979567527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmIAL82ZD5GG"
      },
      "source": [
        "n_comp = 20\n",
        "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
        "svd_obj.fit(train_tfidf)\n",
        "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
        "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
        "    \n",
        "train_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
        "test_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
        "train_df = pd.concat([train_df, train_svd], axis=1)\n",
        "test_df = pd.concat([test_df, test_svd], axis=1)\n",
        "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS9djktSOyRY",
        "outputId": "e3f252d6-7f53-4b19-85d0-7263c81ee603"
      },
      "source": [
        "cols_to_drop = ['index', 'text']\n",
        "train_X = train_df.drop(cols_to_drop+['author'], axis=1)\n",
        "test_X = test_df.drop(cols_to_drop, axis=1)\n",
        "\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 5])\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runLGBM(dev_X, dev_y, val_X, val_y, test_X)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "    \n",
        "print(\"cv scores : \", cv_scores)\n",
        "\n",
        "pred_full_test = pred_full_test / 5.\n",
        "out_df = pd.DataFrame(pred_full_test)\n",
        "out_df.columns = ['0', '1', '2', '3', '4']\n",
        "out_df.insert(0, 'index', test_id)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.87078\tvalid_1's multi_logloss: 0.91588\n",
            "[200]\ttraining's multi_logloss: 0.630619\tvalid_1's multi_logloss: 0.710114\n",
            "[300]\ttraining's multi_logloss: 0.516715\tvalid_1's multi_logloss: 0.629402\n",
            "[400]\ttraining's multi_logloss: 0.446575\tvalid_1's multi_logloss: 0.592726\n",
            "[500]\ttraining's multi_logloss: 0.39463\tvalid_1's multi_logloss: 0.573351\n",
            "[600]\ttraining's multi_logloss: 0.35234\tvalid_1's multi_logloss: 0.56224\n",
            "[700]\ttraining's multi_logloss: 0.316325\tvalid_1's multi_logloss: 0.555248\n",
            "[800]\ttraining's multi_logloss: 0.284971\tvalid_1's multi_logloss: 0.550501\n",
            "[900]\ttraining's multi_logloss: 0.257608\tvalid_1's multi_logloss: 0.547049\n",
            "[1000]\ttraining's multi_logloss: 0.233536\tvalid_1's multi_logloss: 0.544917\n",
            "[1100]\ttraining's multi_logloss: 0.212153\tvalid_1's multi_logloss: 0.543274\n",
            "[1200]\ttraining's multi_logloss: 0.193097\tvalid_1's multi_logloss: 0.542574\n",
            "[1300]\ttraining's multi_logloss: 0.1761\tvalid_1's multi_logloss: 0.542187\n",
            "Early stopping, best iteration is:\n",
            "[1290]\ttraining's multi_logloss: 0.177697\tvalid_1's multi_logloss: 0.542154\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.871841\tvalid_1's multi_logloss: 0.908753\n",
            "[200]\ttraining's multi_logloss: 0.631998\tvalid_1's multi_logloss: 0.702366\n",
            "[300]\ttraining's multi_logloss: 0.518261\tvalid_1's multi_logloss: 0.621275\n",
            "[400]\ttraining's multi_logloss: 0.448427\tvalid_1's multi_logloss: 0.583982\n",
            "[500]\ttraining's multi_logloss: 0.396742\tvalid_1's multi_logloss: 0.564486\n",
            "[600]\ttraining's multi_logloss: 0.354707\tvalid_1's multi_logloss: 0.552883\n",
            "[700]\ttraining's multi_logloss: 0.318762\tvalid_1's multi_logloss: 0.544815\n",
            "[800]\ttraining's multi_logloss: 0.28762\tvalid_1's multi_logloss: 0.539285\n",
            "[900]\ttraining's multi_logloss: 0.260423\tvalid_1's multi_logloss: 0.535832\n",
            "[1000]\ttraining's multi_logloss: 0.236352\tvalid_1's multi_logloss: 0.533092\n",
            "[1100]\ttraining's multi_logloss: 0.214849\tvalid_1's multi_logloss: 0.531221\n",
            "[1200]\ttraining's multi_logloss: 0.195814\tvalid_1's multi_logloss: 0.530211\n",
            "[1300]\ttraining's multi_logloss: 0.178771\tvalid_1's multi_logloss: 0.529549\n",
            "[1400]\ttraining's multi_logloss: 0.163258\tvalid_1's multi_logloss: 0.528883\n",
            "Early stopping, best iteration is:\n",
            "[1402]\ttraining's multi_logloss: 0.162963\tvalid_1's multi_logloss: 0.528878\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.871032\tvalid_1's multi_logloss: 0.914467\n",
            "[200]\ttraining's multi_logloss: 0.631556\tvalid_1's multi_logloss: 0.706919\n",
            "[300]\ttraining's multi_logloss: 0.517917\tvalid_1's multi_logloss: 0.624738\n",
            "[400]\ttraining's multi_logloss: 0.448073\tvalid_1's multi_logloss: 0.586923\n",
            "[500]\ttraining's multi_logloss: 0.396287\tvalid_1's multi_logloss: 0.566587\n",
            "[600]\ttraining's multi_logloss: 0.354086\tvalid_1's multi_logloss: 0.554781\n",
            "[700]\ttraining's multi_logloss: 0.318021\tvalid_1's multi_logloss: 0.547343\n",
            "[800]\ttraining's multi_logloss: 0.286822\tvalid_1's multi_logloss: 0.542016\n",
            "[900]\ttraining's multi_logloss: 0.259517\tvalid_1's multi_logloss: 0.538046\n",
            "[1000]\ttraining's multi_logloss: 0.235436\tvalid_1's multi_logloss: 0.535469\n",
            "[1100]\ttraining's multi_logloss: 0.214117\tvalid_1's multi_logloss: 0.533879\n",
            "[1200]\ttraining's multi_logloss: 0.195011\tvalid_1's multi_logloss: 0.532942\n",
            "[1300]\ttraining's multi_logloss: 0.177794\tvalid_1's multi_logloss: 0.532233\n",
            "[1400]\ttraining's multi_logloss: 0.162382\tvalid_1's multi_logloss: 0.531849\n",
            "[1500]\ttraining's multi_logloss: 0.148446\tvalid_1's multi_logloss: 0.531856\n",
            "Early stopping, best iteration is:\n",
            "[1462]\ttraining's multi_logloss: 0.153567\tvalid_1's multi_logloss: 0.531728\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.873644\tvalid_1's multi_logloss: 0.907933\n",
            "[200]\ttraining's multi_logloss: 0.634771\tvalid_1's multi_logloss: 0.696222\n",
            "[300]\ttraining's multi_logloss: 0.521434\tvalid_1's multi_logloss: 0.611651\n",
            "[400]\ttraining's multi_logloss: 0.451577\tvalid_1's multi_logloss: 0.572233\n",
            "[500]\ttraining's multi_logloss: 0.399772\tvalid_1's multi_logloss: 0.551469\n",
            "[600]\ttraining's multi_logloss: 0.357375\tvalid_1's multi_logloss: 0.53888\n",
            "[700]\ttraining's multi_logloss: 0.321414\tvalid_1's multi_logloss: 0.530591\n",
            "[800]\ttraining's multi_logloss: 0.290073\tvalid_1's multi_logloss: 0.524487\n",
            "[900]\ttraining's multi_logloss: 0.262703\tvalid_1's multi_logloss: 0.520727\n",
            "[1000]\ttraining's multi_logloss: 0.238518\tvalid_1's multi_logloss: 0.518002\n",
            "[1100]\ttraining's multi_logloss: 0.21706\tvalid_1's multi_logloss: 0.516029\n",
            "[1200]\ttraining's multi_logloss: 0.197747\tvalid_1's multi_logloss: 0.514465\n",
            "[1300]\ttraining's multi_logloss: 0.180477\tvalid_1's multi_logloss: 0.513554\n",
            "[1400]\ttraining's multi_logloss: 0.16493\tvalid_1's multi_logloss: 0.513052\n",
            "[1500]\ttraining's multi_logloss: 0.15091\tvalid_1's multi_logloss: 0.51289\n",
            "Early stopping, best iteration is:\n",
            "[1478]\ttraining's multi_logloss: 0.153888\tvalid_1's multi_logloss: 0.512782\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[100]\ttraining's multi_logloss: 0.870053\tvalid_1's multi_logloss: 0.914386\n",
            "[200]\ttraining's multi_logloss: 0.62973\tvalid_1's multi_logloss: 0.709325\n",
            "[300]\ttraining's multi_logloss: 0.516149\tvalid_1's multi_logloss: 0.628772\n",
            "[400]\ttraining's multi_logloss: 0.446162\tvalid_1's multi_logloss: 0.592244\n",
            "[500]\ttraining's multi_logloss: 0.394494\tvalid_1's multi_logloss: 0.572745\n",
            "[600]\ttraining's multi_logloss: 0.352352\tvalid_1's multi_logloss: 0.561235\n",
            "[700]\ttraining's multi_logloss: 0.316478\tvalid_1's multi_logloss: 0.553683\n",
            "[800]\ttraining's multi_logloss: 0.285231\tvalid_1's multi_logloss: 0.548579\n",
            "[900]\ttraining's multi_logloss: 0.258099\tvalid_1's multi_logloss: 0.545225\n",
            "[1000]\ttraining's multi_logloss: 0.234055\tvalid_1's multi_logloss: 0.542778\n",
            "[1100]\ttraining's multi_logloss: 0.212718\tvalid_1's multi_logloss: 0.541102\n",
            "[1200]\ttraining's multi_logloss: 0.193697\tvalid_1's multi_logloss: 0.540129\n",
            "[1300]\ttraining's multi_logloss: 0.176478\tvalid_1's multi_logloss: 0.539561\n",
            "Early stopping, best iteration is:\n",
            "[1311]\ttraining's multi_logloss: 0.174686\tvalid_1's multi_logloss: 0.53949\n",
            "cv scores :  [0.542153742875422, 0.5288779650641494, 0.5317280528340267, 0.5127819519787387, 0.5394901693446577]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKtcH4rx4EiW",
        "outputId": "84a7df95-5b01-45e4-cbd2-d21fe028fcd5"
      },
      "source": [
        "pred_train.shape,pred_full_test.shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((54879, 5), (19617, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej55EFvkJwnB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BrnomynKovC"
      },
      "source": [
        "algo_name = 'LGB'\n",
        "feature_name = 'tfidf'\n",
        "model_name = f'{algo_name}_{feature_name}'\n",
        "\n",
        "feature_file = feature_dir / f'{feature_name}.csv'\n",
        "p_val_file = val_dir / f'{model_name}.val.csv'\n",
        "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
        "sub_file = sub_dir / f'{model_name}.csv'\n",
        "\n",
        "np.savetxt(p_val_file, pred_train, fmt='%.6f', delimiter=',')\n",
        "np.savetxt(p_tst_file, pred_full_test, fmt='%.6f', delimiter=',')"
      ],
      "execution_count": 59,
      "outputs": []
    }
  ]
}