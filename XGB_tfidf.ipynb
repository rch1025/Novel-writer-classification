{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGB tfidf.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joonyoung-Song/DACON-NLP_competition/blob/main/XGB_tfidf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUNLvkRXsjpJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fGTQqubQYf2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import re\n",
        "import os\n",
        "import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "import lightgbm as lgbm\n",
        "import xgboost as xgb\n",
        "\n",
        "from scipy import sparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn import ensemble, metrics, model_selection, naive_bayes\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "from keras.initializers import Constant\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem import PorterStemmer\n",
        "import string"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fExgsc2pytBn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p6MRqPpy5ct",
        "outputId": "6bedfc43-9d2d-4c38-8722-6a527533e63d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjk301MFzBbR"
      },
      "source": [
        "data_dir = Path('/gdrive/My Drive/dacon_nlp_competition/data')\n",
        "feature_dir = Path('/gdrive/My Drive/dacon_nlp_competition/build/feature')\n",
        "val_dir = Path('/gdrive/My Drive/dacon_nlp_competition/build/val')\n",
        "tst_dir = Path('/gdrive/My Drive/dacon_nlp_competition/build/tst')\n",
        "sub_dir = Path('/gdrive/My Drive/dacon_nlp_competition/build/sub')\n",
        "\n",
        "trn_file = data_dir / 'train.csv'\n",
        "tst_file = data_dir / 'test_x.csv'\n",
        "sample_file = data_dir / 'sample_submission.csv'\n",
        "\n",
        "target_col = 'author'\n",
        "n_fold = 5\n",
        "n_class = 5\n",
        "seed = 42"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMcsFNEozB8U"
      },
      "source": [
        "train = pd.read_csv(trn_file, encoding = 'utf-8')\n",
        "test = pd.read_csv(tst_file, encoding = 'utf-8')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xzNkibyQYf5"
      },
      "source": [
        "# train = pd.read_csv('train.csv', encoding = 'utf-8')\n",
        "# test = pd.read_csv('test_x.csv', encoding = 'utf-8')\n",
        "# sample_submission = pd.read_csv('sample_submission.csv', encoding = 'utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RURrZsZB-PHr",
        "outputId": "87c85975-c50b-4150-e94b-e4f6001ab811"
      },
      "source": [
        "# 불용어 불러오기\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english')) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG_CZ9G3-PFq"
      },
      "source": [
        "## Number of words in the text ##\n",
        "train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "test[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "## Number of unique words in the text ##\n",
        "train[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
        "test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
        "\n",
        "## Number of characters in the text ##\n",
        "train[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\n",
        "test[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n",
        "\n",
        "## Number of stopwords in the text ##\n",
        "train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
        "test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
        "\n",
        "## Number of punctuations in the text ##\n",
        "train[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "test[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
        "\n",
        "## Number of title case words in the text ##\n",
        "train[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "\n",
        "## Number of title case words in the text ##\n",
        "train[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
        "test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
        "\n",
        "## Average length of the words in the text ##\n",
        "train[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "test[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFdbpcRP-O5W"
      },
      "source": [
        "## Prepare the data for modeling ###\n",
        "train_y = train['author']\n",
        "train_id = train['index'].values\n",
        "test_id = test['index'].values\n",
        "\n",
        "### recompute the trauncated variables again ###\n",
        "train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "test[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "train[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "test[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "\n",
        "cols_to_drop = ['index', 'text']\n",
        "train_X = train.drop(cols_to_drop + ['author'], axis=1)\n",
        "test_X = test.drop(cols_to_drop, axis=1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmpdM0C3-O2O"
      },
      "source": [
        "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val = 42, child=1, colsample=0.3):\n",
        "    param = {}\n",
        "    param['objective'] = 'multi:softprob'\n",
        "    param['eta'] = 0.1\n",
        "    param['max_depth'] = 3\n",
        "    param['silent'] = 1\n",
        "    param['booster'] = 'dart'\n",
        "    param['num_class'] = 5\n",
        "    param['eval_metric'] = \"mlogloss\"\n",
        "    param['min_child_weight'] = child\n",
        "    param['subsample'] = 0.8\n",
        "    param['colsample_bytree'] = colsample\n",
        "    param['seed'] = seed_val\n",
        "    param['tree_method'] = 'gpu_hist'\n",
        "    num_rounds = 20000\n",
        "\n",
        "    plst = list(param.items())\n",
        "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
        "\n",
        "    if test_y is not None:\n",
        "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
        "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
        "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval= 100)\n",
        "    else:\n",
        "        xgtest = xgb.DMatrix(test_X)\n",
        "        model = xgb.train(plst, xgtrain, num_rounds)\n",
        "\n",
        "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
        "    if test_X2 is not None:\n",
        "        xgtest2 = xgb.DMatrix(test_X2)\n",
        "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
        "    return pred_test_y, pred_test_y2, model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BFNy3nvK4Gv"
      },
      "source": [
        "def runLGBM(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val = 42, child=1):\n",
        "    param = {}\n",
        "    param['objective'] = 'multiclass'\n",
        "    param['boosting_type'] = 'gbdt'\n",
        "    param['subsample_freq'] = 5\n",
        "    param['max_depth'] = 10\n",
        "    param['num_leaves'] = 100\n",
        "    param['num_class'] = 5\n",
        "    param['min_data_in_leaf'] = 64\n",
        "    param['metric'] = 'multi_logloss'\n",
        "    param['subsample_for_bin'] = 23000\n",
        "    param['min_child_weight'] = child\n",
        "    param['learning_rate'] = 0.01\n",
        "    param['seed'] = seed_val\n",
        "    n_estimators = 20000\n",
        "\n",
        "    plst = list(param.items())\n",
        "    lgbmtrain = lgbm.Dataset(train_X, label=train_y, silent = True)\n",
        "\n",
        "    if test_y is not None:\n",
        "        lgbmtest = lgbm.Dataset(test_X, label=test_y, silent = True)\n",
        "        watchlist = [ (lgbmtrain,'train'), (lgbmtest, 'test') ]\n",
        "        model = lgbm.train(plst, lgbmtrain, n_estimators, watchlist, early_stopping_rounds=50, verbose_eval= 20)\n",
        "    else:\n",
        "        lgbmtest = lgbm.Dataset(test_X)\n",
        "        model = lgbm.train(plst, lgbmtrain, num_rounds)\n",
        "\n",
        "    pred_test_y = model.predict(lgbmtest, ntree_limit = model.best_ntree_limit)\n",
        "    if test_X2 is not None:\n",
        "        lgbmtest2 = lgbm.Dataset(test_X2)\n",
        "        pred_test_y2 = model.predict(lgbmtest2, ntree_limit = model.best_ntree_limit)\n",
        "    return pred_test_y, pred_test_y2, model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv0VAAaK-OoE"
      },
      "source": [
        "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
        "    model = naive_bayes.MultinomialNB()\n",
        "    model.fit(train_X, train_y)\n",
        "    pred_test_y = model.predict_proba(test_X)\n",
        "    pred_test_y2 = model.predict_proba(test_X2)\n",
        "    return pred_test_y, pred_test_y2, model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LDB2rSQ-OrX"
      },
      "source": [
        "### Fit transform the tfidf vectorizer ###\n",
        "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
        "full_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75KixRk9D5RV"
      },
      "source": [
        "n_comp = 20\n",
        "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
        "svd_obj.fit(train_tfidf)\n",
        "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
        "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
        "    \n",
        "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
        "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
        "train_df = pd.concat([train, train_svd], axis=1)\n",
        "test_df = pd.concat([test, test_svd], axis=1)\n",
        "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl4imJ-ID5PI"
      },
      "source": [
        "### Fit transform the count vectorizer ###\n",
        "tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
        "tfidf_vec.fit(train_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acdwasGnFh9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b81f44b4-a72d-4e5a-c37a-c0404aed2c8c"
      },
      "source": [
        "### Fit transform the tfidf vectorizer ###\n",
        "tfidf_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
        "tfidf_vec.fit(train_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 5])\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state = 42)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "# add the predictions as new features #\n",
        "train_df[\"nb_cvec_char_eap\"] = pred_train[:,0]\n",
        "train_df[\"nb_cvec_char_hpl\"] = pred_train[:,1]\n",
        "train_df[\"nb_cvec_char_mws\"] = pred_train[:,2]\n",
        "test_df[\"nb_cvec_char_eap\"] = pred_full_test[:,0]\n",
        "test_df[\"nb_cvec_char_hpl\"] = pred_full_test[:,1]\n",
        "test_df[\"nb_cvec_char_mws\"] = pred_full_test[:,2]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean cv score :  5.340966334159593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBqCyAIYD5IJ",
        "outputId": "39903fbc-823b-4704-87e5-9e57516b5928"
      },
      "source": [
        "### Fit transform the tfidf vectorizer ###\n",
        "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\n",
        "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist())\n",
        "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
        "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
        "\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 5])\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state= 42)\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "print(\"Mean cv score : \", np.mean(cv_scores))\n",
        "pred_full_test = pred_full_test / 5.\n",
        "\n",
        "# add the predictions as new features #\n",
        "train_df[\"nb_tfidf_char_eap\"] = pred_train[:,0]\n",
        "train_df[\"nb_tfidf_char_hpl\"] = pred_train[:,1]\n",
        "train_df[\"nb_tfidf_char_mws\"] = pred_train[:,2]\n",
        "test_df[\"nb_tfidf_char_eap\"] = pred_full_test[:,0]\n",
        "test_df[\"nb_tfidf_char_hpl\"] = pred_full_test[:,1]\n",
        "test_df[\"nb_tfidf_char_mws\"] = pred_full_test[:,2]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean cv score :  1.6966907979567527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmIAL82ZD5GG"
      },
      "source": [
        "n_comp = 20\n",
        "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
        "svd_obj.fit(train_tfidf)\n",
        "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
        "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
        "    \n",
        "train_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
        "test_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
        "train_df = pd.concat([train_df, train_svd], axis=1)\n",
        "test_df = pd.concat([test_df, test_svd], axis=1)\n",
        "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS9djktSOyRY",
        "outputId": "45abc6c3-f86a-4733-ee15-83c860f3df3c"
      },
      "source": [
        "cols_to_drop = ['index', 'text']\n",
        "train_X = train_df.drop(cols_to_drop+['author'], axis=1)\n",
        "test_X = test_df.drop(cols_to_drop, axis=1)\n",
        "\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = []\n",
        "pred_full_test = 0\n",
        "pred_train = np.zeros([train_df.shape[0], 5])\n",
        "for dev_index, val_index in kf.split(train_X):\n",
        "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n",
        "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
        "    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val= 42, colsample=0.7)\n",
        "    pred_full_test = pred_full_test + pred_test_y\n",
        "    pred_train[val_index,:] = pred_val_y\n",
        "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
        "    \n",
        "print(\"cv scores : \", cv_scores)\n",
        "\n",
        "pred_full_test = pred_full_test / 5.\n",
        "out_df = pd.DataFrame(pred_full_test)\n",
        "out_df.columns = ['0', '1', '2', '3', '4']\n",
        "out_df.insert(0, 'index', test_id)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\ttrain-mlogloss:1.49198\ttest-mlogloss:1.49243\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[100]\ttrain-mlogloss:0.580782\ttest-mlogloss:0.609423\n",
            "[200]\ttrain-mlogloss:0.52529\ttest-mlogloss:0.581017\n",
            "[300]\ttrain-mlogloss:0.489663\ttest-mlogloss:0.567775\n",
            "[400]\ttrain-mlogloss:0.461411\ttest-mlogloss:0.560199\n",
            "[500]\ttrain-mlogloss:0.437539\ttest-mlogloss:0.555105\n",
            "[600]\ttrain-mlogloss:0.416412\ttest-mlogloss:0.551847\n",
            "[700]\ttrain-mlogloss:0.397095\ttest-mlogloss:0.549668\n",
            "[800]\ttrain-mlogloss:0.378991\ttest-mlogloss:0.548045\n",
            "[900]\ttrain-mlogloss:0.362812\ttest-mlogloss:0.547463\n",
            "[1000]\ttrain-mlogloss:0.347052\ttest-mlogloss:0.546746\n",
            "Stopping. Best iteration:\n",
            "[987]\ttrain-mlogloss:0.349157\ttest-mlogloss:0.546497\n",
            "\n",
            "[0]\ttrain-mlogloss:1.49171\ttest-mlogloss:1.4913\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[100]\ttrain-mlogloss:0.581115\ttest-mlogloss:0.608435\n",
            "[200]\ttrain-mlogloss:0.526789\ttest-mlogloss:0.576423\n",
            "[300]\ttrain-mlogloss:0.49154\ttest-mlogloss:0.560996\n",
            "[400]\ttrain-mlogloss:0.463921\ttest-mlogloss:0.551386\n",
            "[500]\ttrain-mlogloss:0.440316\ttest-mlogloss:0.545394\n",
            "[600]\ttrain-mlogloss:0.41927\ttest-mlogloss:0.541228\n",
            "[700]\ttrain-mlogloss:0.399919\ttest-mlogloss:0.537542\n",
            "[800]\ttrain-mlogloss:0.382354\ttest-mlogloss:0.535186\n",
            "[900]\ttrain-mlogloss:0.365673\ttest-mlogloss:0.533063\n",
            "[1000]\ttrain-mlogloss:0.350105\ttest-mlogloss:0.531705\n",
            "[1100]\ttrain-mlogloss:0.335518\ttest-mlogloss:0.531008\n",
            "[1200]\ttrain-mlogloss:0.321848\ttest-mlogloss:0.530431\n",
            "[1300]\ttrain-mlogloss:0.308697\ttest-mlogloss:0.5297\n",
            "[1400]\ttrain-mlogloss:0.296277\ttest-mlogloss:0.529334\n",
            "Stopping. Best iteration:\n",
            "[1372]\ttrain-mlogloss:0.299728\ttest-mlogloss:0.529229\n",
            "\n",
            "[0]\ttrain-mlogloss:1.49128\ttest-mlogloss:1.4924\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[100]\ttrain-mlogloss:0.580516\ttest-mlogloss:0.612535\n",
            "[200]\ttrain-mlogloss:0.526854\ttest-mlogloss:0.581539\n",
            "[300]\ttrain-mlogloss:0.491194\ttest-mlogloss:0.566671\n",
            "[400]\ttrain-mlogloss:0.463117\ttest-mlogloss:0.557985\n",
            "[500]\ttrain-mlogloss:0.439231\ttest-mlogloss:0.551152\n",
            "[600]\ttrain-mlogloss:0.418461\ttest-mlogloss:0.547272\n",
            "[700]\ttrain-mlogloss:0.398906\ttest-mlogloss:0.544074\n",
            "[800]\ttrain-mlogloss:0.38098\ttest-mlogloss:0.542702\n",
            "[900]\ttrain-mlogloss:0.364042\ttest-mlogloss:0.540738\n",
            "[1000]\ttrain-mlogloss:0.348476\ttest-mlogloss:0.539427\n",
            "[1100]\ttrain-mlogloss:0.333811\ttest-mlogloss:0.538703\n",
            "Stopping. Best iteration:\n",
            "[1067]\ttrain-mlogloss:0.338462\ttest-mlogloss:0.538652\n",
            "\n",
            "[0]\ttrain-mlogloss:1.49176\ttest-mlogloss:1.49204\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[100]\ttrain-mlogloss:0.585483\ttest-mlogloss:0.592519\n",
            "[200]\ttrain-mlogloss:0.531022\ttest-mlogloss:0.56148\n",
            "[300]\ttrain-mlogloss:0.495196\ttest-mlogloss:0.54691\n",
            "[400]\ttrain-mlogloss:0.466738\ttest-mlogloss:0.537845\n",
            "[500]\ttrain-mlogloss:0.442701\ttest-mlogloss:0.532075\n",
            "[600]\ttrain-mlogloss:0.421374\ttest-mlogloss:0.52872\n",
            "[700]\ttrain-mlogloss:0.40217\ttest-mlogloss:0.526271\n",
            "[800]\ttrain-mlogloss:0.384274\ttest-mlogloss:0.524115\n",
            "[900]\ttrain-mlogloss:0.3675\ttest-mlogloss:0.522679\n",
            "[1000]\ttrain-mlogloss:0.351914\ttest-mlogloss:0.521319\n",
            "[1100]\ttrain-mlogloss:0.337247\ttest-mlogloss:0.52036\n",
            "[1200]\ttrain-mlogloss:0.323521\ttest-mlogloss:0.520094\n",
            "Stopping. Best iteration:\n",
            "[1190]\ttrain-mlogloss:0.32489\ttest-mlogloss:0.519941\n",
            "\n",
            "[0]\ttrain-mlogloss:1.49078\ttest-mlogloss:1.49189\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
            "[100]\ttrain-mlogloss:0.578893\ttest-mlogloss:0.616397\n",
            "[200]\ttrain-mlogloss:0.524453\ttest-mlogloss:0.58404\n",
            "[300]\ttrain-mlogloss:0.48853\ttest-mlogloss:0.568429\n",
            "[400]\ttrain-mlogloss:0.460556\ttest-mlogloss:0.558413\n",
            "[500]\ttrain-mlogloss:0.437348\ttest-mlogloss:0.553637\n",
            "[600]\ttrain-mlogloss:0.416675\ttest-mlogloss:0.550049\n",
            "[700]\ttrain-mlogloss:0.397332\ttest-mlogloss:0.547059\n",
            "[800]\ttrain-mlogloss:0.379658\ttest-mlogloss:0.544994\n",
            "[900]\ttrain-mlogloss:0.363672\ttest-mlogloss:0.543241\n",
            "[1000]\ttrain-mlogloss:0.347754\ttest-mlogloss:0.542415\n",
            "[1100]\ttrain-mlogloss:0.332934\ttest-mlogloss:0.542236\n",
            "Stopping. Best iteration:\n",
            "[1074]\ttrain-mlogloss:0.336672\ttest-mlogloss:0.541982\n",
            "\n",
            "cv scores :  [0.5464972199704159, 0.5292292362574749, 0.5386518763258383, 0.519941459059882, 0.5419820994327426]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulAjH8LB3mzj"
      },
      "source": [
        "# 제출 파일 생성"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-11-09T08:51:19.093Z"
        },
        "id": "3jXKoOWIC6g0"
      },
      "source": [
        "algo_name = 'XGB'\n",
        "feature_name = 'tfidf'\n",
        "model_name = f'{algo_name}_{feature_name}'\n",
        "\n",
        "feature_file = feature_dir / f'{feature_name}.csv'\n",
        "p_val_file = val_dir / f'{model_name}.val.csv'\n",
        "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
        "sub_file = sub_dir / f'{model_name}.csv'\n",
        "\n",
        "np.savetxt(p_val_file, pred_train, fmt='%.6f', delimiter=',')\n",
        "np.savetxt(p_tst_file, pred_full_test, fmt='%.6f', delimiter=',')"
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}