{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords  \n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn=pd.read_csv('data/train.csv',index_col=0)\n",
    "tst=pd.read_csv('data/test_x.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    15063\n",
       "0    13235\n",
       "2    11554\n",
       "4     7805\n",
       "1     7222\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn['author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was almost choking. There was so much, so much he wanted to say, but strange exclamations were all that came from his lips. The Pole gazed fixedly at him, at the bundle of notes in his hand; looked at odin, and was in evident perplexity.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class=5\n",
    "n_fold=5\n",
    "seed=32152339\n",
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn[\"token\"]=trn[\"text\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatizaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATIV\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(trn['token'])):\n",
    "    trn['token'][i]=[lemmatizer.lemmatize(t) for t in trn['token'][i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer로 token화, 불용어제거, 실수형변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATIV\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2911)\n"
     ]
    }
   ],
   "source": [
    "vec=TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'),min_df=50)\n",
    "X_cnt=vec.fit_transform(trn['text'])\n",
    "print(X_cnt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19617, 2379)\n"
     ]
    }
   ],
   "source": [
    "X_cnt_tst=vec.fit_transform(tst['text'])\n",
    "print(X_cnt_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.18298108,\n",
       "         0.18845654],\n",
       "        [0.        , 0.        , 0.        , ..., 0.08014345, 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.20204159,\n",
       "         0.20808741],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cnt.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(units=64, activation='relu'))\n",
    "    model.add(Dense(units=n_class, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn,X_val,y_trn,y_val = train_test_split(X_cnt,trn['author'],test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38415 samples, validate on 16464 samples\n",
      "Epoch 1/2\n",
      "38415/38415 [==============================] - ETA: 13:24:18 - loss: 1.611 - ETA: 6:42:19 - loss: 1.609 - ETA: 2:13:50 - loss: 1.61 - ETA: 1:12:47 - loss: 1.60 - ETA: 49:54 - loss: 1.6036 - ETA: 34:33 - loss: 1.60 - ETA: 27:18 - loss: 1.59 - ETA: 21:16 - loss: 1.59 - ETA: 17:24 - loss: 1.58 - ETA: 14:41 - loss: 1.58 - ETA: 12:41 - loss: 1.57 - ETA: 11:09 - loss: 1.56 - ETA: 9:56 - loss: 1.5625 - ETA: 9:04 - loss: 1.557 - ETA: 8:13 - loss: 1.553 - ETA: 7:36 - loss: 1.547 - ETA: 7:04 - loss: 1.541 - ETA: 6:32 - loss: 1.535 - ETA: 6:07 - loss: 1.530 - ETA: 5:42 - loss: 1.523 - ETA: 5:25 - loss: 1.519 - ETA: 5:08 - loss: 1.512 - ETA: 4:52 - loss: 1.506 - ETA: 4:35 - loss: 1.499 - ETA: 4:18 - loss: 1.492 - ETA: 4:03 - loss: 1.483 - ETA: 3:51 - loss: 1.477 - ETA: 3:38 - loss: 1.468 - ETA: 3:26 - loss: 1.458 - ETA: 3:15 - loss: 1.449 - ETA: 3:09 - loss: 1.442 - ETA: 3:03 - loss: 1.436 - ETA: 2:57 - loss: 1.430 - ETA: 2:51 - loss: 1.422 - ETA: 2:44 - loss: 1.414 - ETA: 2:37 - loss: 1.403 - ETA: 2:30 - loss: 1.393 - ETA: 2:24 - loss: 1.383 - ETA: 2:18 - loss: 1.374 - ETA: 2:12 - loss: 1.365 - ETA: 2:08 - loss: 1.358 - ETA: 2:02 - loss: 1.349 - ETA: 1:57 - loss: 1.340 - ETA: 1:53 - loss: 1.330 - ETA: 1:48 - loss: 1.322 - ETA: 1:43 - loss: 1.313 - ETA: 1:40 - loss: 1.303 - ETA: 1:35 - loss: 1.294 - ETA: 1:32 - loss: 1.284 - ETA: 1:28 - loss: 1.275 - ETA: 1:25 - loss: 1.267 - ETA: 1:21 - loss: 1.258 - ETA: 1:18 - loss: 1.251 - ETA: 1:15 - loss: 1.243 - ETA: 1:12 - loss: 1.234 - ETA: 1:10 - loss: 1.229 - ETA: 1:07 - loss: 1.222 - ETA: 1:05 - loss: 1.216 - ETA: 1:02 - loss: 1.207 - ETA: 1:00 - loss: 1.201 - ETA: 58s - loss: 1.194 - ETA: 56s - loss: 1.18 - ETA: 54s - loss: 1.18 - ETA: 52s - loss: 1.17 - ETA: 50s - loss: 1.16 - ETA: 48s - loss: 1.16 - ETA: 46s - loss: 1.15 - ETA: 44s - loss: 1.14 - ETA: 42s - loss: 1.14 - ETA: 41s - loss: 1.13 - ETA: 39s - loss: 1.13 - ETA: 38s - loss: 1.12 - ETA: 37s - loss: 1.12 - ETA: 35s - loss: 1.11 - ETA: 34s - loss: 1.11 - ETA: 32s - loss: 1.10 - ETA: 31s - loss: 1.09 - ETA: 30s - loss: 1.09 - ETA: 28s - loss: 1.08 - ETA: 27s - loss: 1.08 - ETA: 26s - loss: 1.07 - ETA: 25s - loss: 1.07 - ETA: 24s - loss: 1.07 - ETA: 23s - loss: 1.06 - ETA: 22s - loss: 1.06 - ETA: 20s - loss: 1.05 - ETA: 19s - loss: 1.05 - ETA: 19s - loss: 1.05 - ETA: 18s - loss: 1.04 - ETA: 17s - loss: 1.04 - ETA: 16s - loss: 1.03 - ETA: 15s - loss: 1.03 - ETA: 14s - loss: 1.03 - ETA: 13s - loss: 1.02 - ETA: 12s - loss: 1.02 - ETA: 11s - loss: 1.01 - ETA: 11s - loss: 1.01 - ETA: 10s - loss: 1.01 - ETA: 9s - loss: 1.0105 - ETA: 9s - loss: 1.006 - ETA: 8s - loss: 1.003 - ETA: 7s - loss: 1.000 - ETA: 6s - loss: 0.996 - ETA: 6s - loss: 0.993 - ETA: 5s - loss: 0.991 - ETA: 5s - loss: 0.988 - ETA: 4s - loss: 0.985 - ETA: 3s - loss: 0.982 - ETA: 3s - loss: 0.979 - ETA: 2s - loss: 0.976 - ETA: 2s - loss: 0.973 - ETA: 1s - loss: 0.971 - ETA: 0s - loss: 0.968 - ETA: 0s - loss: 0.965 - 69s 2ms/sample - loss: 0.9631 - val_loss: 0.7429\n",
      "Epoch 2/2\n",
      "38415/38415 [==============================] - ETA: 1:27 - loss: 0.618 - ETA: 18s - loss: 0.581 - ETA: 13s - loss: 0.62 - ETA: 11s - loss: 0.63 - ETA: 10s - loss: 0.62 - ETA: 9s - loss: 0.6250 - ETA: 9s - loss: 0.627 - ETA: 8s - loss: 0.614 - ETA: 8s - loss: 0.611 - ETA: 8s - loss: 0.615 - ETA: 8s - loss: 0.618 - ETA: 8s - loss: 0.617 - ETA: 7s - loss: 0.630 - ETA: 7s - loss: 0.630 - ETA: 7s - loss: 0.631 - ETA: 7s - loss: 0.635 - ETA: 7s - loss: 0.632 - ETA: 7s - loss: 0.635 - ETA: 6s - loss: 0.638 - ETA: 6s - loss: 0.635 - ETA: 6s - loss: 0.634 - ETA: 6s - loss: 0.638 - ETA: 6s - loss: 0.640 - ETA: 6s - loss: 0.643 - ETA: 6s - loss: 0.643 - ETA: 6s - loss: 0.644 - ETA: 6s - loss: 0.644 - ETA: 5s - loss: 0.643 - ETA: 5s - loss: 0.643 - ETA: 5s - loss: 0.647 - ETA: 5s - loss: 0.645 - ETA: 5s - loss: 0.644 - ETA: 5s - loss: 0.643 - ETA: 5s - loss: 0.641 - ETA: 5s - loss: 0.642 - ETA: 5s - loss: 0.643 - ETA: 5s - loss: 0.644 - ETA: 4s - loss: 0.645 - ETA: 4s - loss: 0.643 - ETA: 4s - loss: 0.642 - ETA: 4s - loss: 0.641 - ETA: 4s - loss: 0.643 - ETA: 4s - loss: 0.642 - ETA: 4s - loss: 0.642 - ETA: 4s - loss: 0.642 - ETA: 4s - loss: 0.642 - ETA: 4s - loss: 0.642 - ETA: 4s - loss: 0.644 - ETA: 4s - loss: 0.644 - ETA: 4s - loss: 0.644 - ETA: 3s - loss: 0.643 - ETA: 3s - loss: 0.642 - ETA: 3s - loss: 0.643 - ETA: 3s - loss: 0.643 - ETA: 3s - loss: 0.644 - ETA: 3s - loss: 0.645 - ETA: 3s - loss: 0.647 - ETA: 3s - loss: 0.648 - ETA: 3s - loss: 0.647 - ETA: 3s - loss: 0.648 - ETA: 3s - loss: 0.648 - ETA: 3s - loss: 0.649 - ETA: 3s - loss: 0.650 - ETA: 3s - loss: 0.649 - ETA: 2s - loss: 0.648 - ETA: 2s - loss: 0.649 - ETA: 2s - loss: 0.650 - ETA: 2s - loss: 0.649 - ETA: 2s - loss: 0.649 - ETA: 2s - loss: 0.649 - ETA: 2s - loss: 0.649 - ETA: 2s - loss: 0.650 - ETA: 2s - loss: 0.651 - ETA: 2s - loss: 0.651 - ETA: 2s - loss: 0.651 - ETA: 2s - loss: 0.651 - ETA: 2s - loss: 0.652 - ETA: 2s - loss: 0.652 - ETA: 2s - loss: 0.652 - ETA: 1s - loss: 0.652 - ETA: 1s - loss: 0.652 - ETA: 1s - loss: 0.653 - ETA: 1s - loss: 0.654 - ETA: 1s - loss: 0.654 - ETA: 1s - loss: 0.655 - ETA: 1s - loss: 0.655 - ETA: 1s - loss: 0.656 - ETA: 1s - loss: 0.657 - ETA: 1s - loss: 0.657 - ETA: 1s - loss: 0.656 - ETA: 1s - loss: 0.657 - ETA: 1s - loss: 0.657 - ETA: 1s - loss: 0.658 - ETA: 1s - loss: 0.658 - ETA: 1s - loss: 0.658 - ETA: 0s - loss: 0.659 - ETA: 0s - loss: 0.659 - ETA: 0s - loss: 0.659 - ETA: 0s - loss: 0.659 - ETA: 0s - loss: 0.660 - ETA: 0s - loss: 0.660 - ETA: 0s - loss: 0.660 - ETA: 0s - loss: 0.660 - ETA: 0s - loss: 0.660 - ETA: 0s - loss: 0.660 - ETA: 0s - loss: 0.660 - ETA: 0s - loss: 0.661 - ETA: 0s - loss: 0.661 - ETA: 0s - loss: 0.661 - ETA: 0s - loss: 0.661 - ETA: 0s - loss: 0.661 - 8s 198us/sample - loss: 0.6610 - val_loss: 0.7205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aeacb8b0c8>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = get_model()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5,\n",
    "                   verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
    "\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                        patience=3, min_lr=1e-6, mode='min', verbose=1)\n",
    "\n",
    "clf.fit(X_trn.todense(), \n",
    "        to_categorical(y_trn),\n",
    "        validation_data=(X_val.todense(), to_categorical(y_val)),\n",
    "        epochs=2, # 에포크는 일반적으로 10 정도로 둠\n",
    "        callbacks=[es, rlr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val= clf.predict(X_val.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16464"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argmax(p_val,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7205332337415213"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_val,p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7298955296404276"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pd.Series(np.array(y_val)),pd.Series(np.argmax(p_val,axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
